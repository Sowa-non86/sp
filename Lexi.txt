#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <ctype.h>

// Define maximum limits
#define MAX_TOKENS 1000
#define MAX_LENGTH 100

// Token structure
typedef struct {
    int line_number;
    char lexeme[MAX_LENGTH];
    char type[MAX_LENGTH];
    int token_value;
} Token;

// Token categories
char keywords[MAX_TOKENS][MAX_LENGTH];
char identifiers[MAX_TOKENS][MAX_LENGTH];
char numbers[MAX_TOKENS][MAX_LENGTH];
char operators[MAX_TOKENS][MAX_LENGTH];
char punctuators[MAX_TOKENS][MAX_LENGTH];
int keyword_count = 0, identifier_count = 0, number_count = 0, operator_count = 0, punctuator_count = 0;

// Function prototypes
void analyze_file(const char *filename);
void tokenize_line(const char *line, int line_number, Token tokens[], int *token_count);
int is_keyword(const char *lexeme);
int add_to_specific_array(const char *token_type, const char *lexeme);
void print_tokens(const Token tokens[], int token_count);

int main() {
    const char *filename = "lexi.c"; // Change this to your input file
    analyze_file(filename);
    return 0;
}

void analyze_file(const char *filename) {
    FILE *file = fopen(filename, "r");
    if (!file) {
        perror("Error reading file");
        return;
    }

    char line[MAX_LENGTH];
    int line_number = 1;
    Token tokens[MAX_TOKENS];
    int token_count = 0;

    while (fgets(line, sizeof(line), file)) {
        tokenize_line(line, line_number, tokens, &token_count);
        line_number++;
    }

    fclose(file);
    print_tokens(tokens, token_count);
}

void tokenize_line(const char *line, int line_number, Token tokens[], int *token_count) {
    const char *delimiters = " \t\n";
    char *line_copy = strdup(line);
    char *token = strtok(line_copy, delimiters);

    while (token != NULL) {
        Token new_token;
        new_token.line_number = line_number;
        strcpy(new_token.lexeme, token);

        // Determine token type
        if (is_keyword(token)) {
            strcpy(new_token.type, "KEYWORD");
            new_token.token_value = add_to_specific_array("KEYWORD", token);
        } else if (isalpha(token[0]) || token[0] == '_') {
            strcpy(new_token.type, "IDENTIFIER");
            new_token.token_value = add_to_specific_array("IDENTIFIER", token);
        } else if (isdigit(token[0])) {
            strcpy(new_token.type, "NUMBER");
            new_token.token_value = add_to_specific_array("NUMBER", token);
        } else if (strchr("+-*/%=", token[0])) {
            strcpy(new_token.type, "OPERATOR");
            new_token.token_value = add_to_specific_array("OPERATOR", token);
        } else if (strchr("{}();,", token[0])) {
            strcpy(new_token.type, "PUNCTUATOR");
            new_token.token_value = add_to_specific_array("PUNCTUATOR", token);
        } else {
            printf("Error: Unexpected character '%s' on line %d\n", token, line_number);
            token = strtok(NULL, delimiters);
            continue;
        }

        tokens[(*token_count)++] = new_token;
        token = strtok(NULL, delimiters);
    }

    free(line_copy);
}

int is_keyword(const char *lexeme) {
    const char *keywords[] = {"int", "float", "if", "else", "while", "return"};
    for (int i = 0; i < 6; i++) {
        if (strcmp(lexeme, keywords[i]) == 0) {
            return 1;
        }
    }
    return 0;
}

int add_to_specific_array(const char *token_type, const char *lexeme) {
    char (*target_list)[MAX_LENGTH];
    int *count;

    if (strcmp(token_type, "KEYWORD") == 0) {
        target_list = keywords;
        count = &keyword_count;
    } else if (strcmp(token_type, "IDENTIFIER") == 0) {
        target_list = identifiers;
        count = &identifier_count;
    } else if (strcmp(token_type, "NUMBER") == 0) {
        target_list = numbers;
        count = &number_count;
    } else if (strcmp(token_type, "OPERATOR") == 0) {
        target_list = operators;
        count = &operator_count;
    } else if (strcmp(token_type, "PUNCTUATOR") == 0) {
        target_list = punctuators;
        count = &punctuator_count;
    } else {
        fprintf(stderr, "Unexpected token type: %s\n", token_type);
        exit(1);
    }

    strcpy(target_list[*count], lexeme);
    return ++(*count); // Return 1-based index
}

void print_tokens(const Token tokens[], int token_count) {
    printf("%-5s %-15s %-15s %-15s\n", "Line", "Lexeme", "Token Type", "Token Value");
    printf("------------------------------------------------------------\n");
    for (int i = 0; i < token_count; i++) {
        printf("%-5d %-15s %-15s %-15d\n",
               tokens[i].line_number,
               tokens[i].lexeme,
               tokens[i].type,
               tokens[i].token_value);
    }
}
